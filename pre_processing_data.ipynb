{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho dos dados.\n",
    "DTRAIN_PATH = \"data/train.csv\"\n",
    "DTEST_PATH = \"data/test.csv\"\n",
    "\n",
    "# Funções auxiliares.\n",
    "def reading_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def rmse_cv(model, x_train, y_train, k_folds=10):\n",
    "    return np.sqrt(-cross_val_score(model, x_train, y_train,\n",
    "        scoring=\"neg_mean_squared_error\", cv=k_folds))\n",
    "\n",
    "def grid_search(model, params, train_x, train_y):\n",
    "\t# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\t# gridsearch = GridSearchCV(model, param_grid=params, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "\n",
    "\tgridsearch = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "\tgridresult = gridsearch.fit(train_x, train_y)\n",
    "\n",
    "\treturn gridresult\n",
    "\n",
    "def print_grid_result(name, grid_result):\n",
    "\tprint(name + \":\")\n",
    "\t# sumarize results\n",
    "\tprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\tmeans = grid_result.cv_results_['mean_test_score']\n",
    "\tstds = grid_result.cv_results_['std_test_score']\n",
    "\tparams = grid_result.cv_results_['params']\n",
    "\tfor mean, stdev, param in zip(means, stds, params):\n",
    "\t\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\tprint(\"\\n\")\n",
    "\n",
    "\n",
    "def grid_search_lasso(train_x, train_y):\n",
    "\tparams = {'alpha':(1, 0.1, 0.001, 0.0005)}\n",
    "\n",
    "\tgrid_result = grid_search(Lasso(), params, train_x, train_y)\n",
    "\tprint_grid_result(\"Lasso\", grid_result)\n",
    "\n",
    "def grid_search_ridge(train_x, train_y):\n",
    "\tparams = {'alpha':(1, 3, 5, 10, 15, 30, 50, 75)}\n",
    "\n",
    "\tgrid_result = grid_search(Ridge(), params, train_x, train_y)\n",
    "\tprint_grid_result(\"Ridge\", grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções interessantes\n",
    "# all_data.head()\n",
    "# all_data.tail()\n",
    "# all_data.info()\n",
    "# all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns that the number of nans values is greater than bound=0.3 \n",
    "def remove_nan_columns(df, bound=0.3):\n",
    "    for column_name in df.columns:\n",
    "        column = df[column_name]\n",
    "        nan_percentage = column.isnull().sum()/column.size\n",
    "        if(nan_percentage > bound):\n",
    "            df = df.drop(columns=[column_name])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalise a column of values to lie between 0 and 1\n",
    "def scale_minmax(col):\n",
    "    return (col-col.min())/(col.max()-col.min())\n",
    "\n",
    "# treating data with NaNs values\n",
    "def fill_columns_with_NaN(df):\n",
    "    # columns where NaN values have meaning, e.g. no pool, etc\n",
    "    cols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n",
    "               'GarageQual','GarageCond','GarageFinish','GarageType',\n",
    "               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\n",
    "    \n",
    "    # replacing NaN with None\n",
    "    for col in cols_fillna:\n",
    "        df[col].fillna('None', inplace=True)\n",
    "\n",
    "    # GarageYrBlt nans: no garage, fill with property YearBuilt (zero isn't better?)\n",
    "    df.loc[df.GarageYrBlt.isnull(), 'GarageYrBlt'] = df.loc[df.GarageYrBlt.isnull(), 'YearBuilt']\n",
    " \n",
    "    # No mansonry veneer - fill with zeros.\n",
    "    df.MasVnrArea.fillna(0, inplace=True)\n",
    "        \n",
    "    \n",
    "    # No basement - fill areas/counts with 0\n",
    "    df.BsmtFullBath.fillna(0,inplace=True)\n",
    "    df.BsmtHalfBath.fillna(0,inplace=True)\n",
    "    df.BsmtFinSF1.fillna(0,inplace=True)\n",
    "    df.BsmtFinSF2.fillna(0,inplace=True)\n",
    "    df.BsmtUnfSF.fillna(0,inplace=True)\n",
    "    df.TotalBsmtSF.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "    # No garage - fill areas/counts with 0\n",
    "    df.GarageArea.fillna(0,inplace=True)\n",
    "    df.GarageCars.fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "    # convert categoricals to dummies, exclude SalePrice from model\n",
    "    df_frontage = pd.get_dummies(df)\n",
    "    \n",
    "    # normalise columns to 0-1\n",
    "    for col in df_frontage.drop('LotFrontage',axis=1).columns:\n",
    "        df_frontage[col] = scale_minmax(df_frontage[col])\n",
    "    \n",
    "    lf_train = df_frontage.dropna()\n",
    "    lf_train_y = lf_train.LotFrontage\n",
    "    lf_train_X = lf_train.drop('LotFrontage',axis=1)\n",
    "    \n",
    "    lr = Ridge()\n",
    "    lr.fit(lf_train_X, lf_train_y)\n",
    "    \n",
    "\n",
    "    # fill na values using model predictions\n",
    "    nan_frontage = df.LotFrontage.isnull()\n",
    "    X = df_frontage[nan_frontage].drop('LotFrontage',axis=1)\n",
    "    y = lr.predict(X)\n",
    "    # fill nan values of lot frontage\n",
    "    df.loc[nan_frontage,'LotFrontage'] = y\n",
    "    \n",
    "    # Remaining Nan values\n",
    "    cols_with_na = df.isnull().sum()\n",
    "    cols_with_na = cols_with_na[cols_with_na > 0]\n",
    "\n",
    "\n",
    "    # fill remaining nans with mode in that column\n",
    "    for col in cols_with_na.index:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    # fill remaining nans with mean in that column\n",
    "    # df = df.fillna(all_data.mean())\n",
    "    # fill remaining nans with median in that column\n",
    "    # df = df.fillna(all_data.median())\n",
    "\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows that the number of nans values is greater than bound=0.1 (more than 10 values null)\n",
    "def remove_nan_rows(df, bound=0.1):\n",
    "    row_size = train_data.shape[1]\n",
    "    for index, row in df.iterrows():\n",
    "        rownan_percentage = row.isnull().sum()/row_size\n",
    "        if (rownan_percentage >= bound ):\n",
    "            df = df.drop(index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing redundance using drop_duplicates and correlation\n",
    "def remove_redundance(df, bound=0.0005):\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    corr = df.corr().abs()\n",
    "    corr = corr[corr!= 1]\n",
    "    print(corr.min() < bound)\n",
    "    remove = np.empty(0)\n",
    "    columns = corr.columns\n",
    "    for index, row in corr.iterrows():\n",
    "        for col in columns:\n",
    "            if (row[col] < bound):\n",
    "                remove = np.append(remove, col)\n",
    "    print(df.shape)\n",
    "    df = df.drop(columns=np.unique(remove))\n",
    "    print(df.shape)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basement_finish_types(df): \n",
    "    # create separate columns for area of each possible\n",
    "    # basement finish type\n",
    "    bsmt_fin_cols = ['BsmtGLQ','BsmtALQ','BsmtBLQ',\n",
    "                     'BsmtRec','BsmtLwQ']\n",
    "\n",
    "    for col in bsmt_fin_cols:\n",
    "        # initialise as columns of zeros\n",
    "        df[col+'SF'] = 0\n",
    "\n",
    "    # fill remaining finish type columns\n",
    "    for row in df.index:\n",
    "        fin1 = df.loc[row, 'BsmtFinType1']\n",
    "\n",
    "        if (fin1 != 'None') and (fin1 != 'Unf'):\n",
    "            # add area (SF) to appropriate column\n",
    "            df.loc[row, 'Bsmt' + fin1 + 'SF'] += df.loc[row,' BsmtFinSF1']\n",
    "\n",
    "        fin2 = df_all.loc[row, 'BsmtFinType2']\n",
    "        if (fin2 != 'None') and (fin2 != 'Unf'):\n",
    "            df.loc[row, 'Bsmt' + fin2 + 'SF'] += df.loc[row, 'BsmtFinSF2']\n",
    "\n",
    "    # remove initial BsmtFin columns\n",
    "    df.drop(['BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2'], axis=1, inplace=True)\n",
    "\n",
    "    # already have BsmtUnf column in dataset\n",
    "    bsmt_fin_cols.append('BsmtUnf')\n",
    "\n",
    "    # also create features representing the fraction of the basement that is each finish type\n",
    "    for col in bsmt_fin_cols:\n",
    "        df[col + 'Frac'] = df[col + 'SF'] / df['TotalBsmtSF']\n",
    "        # replace any nans with zero (for properties without a basement)\n",
    "        df[col + 'Frac'].fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floor_area(df):\n",
    "    df['LowQualFinFrac'] = df['LowQualFinSF']/df['GrLivArea']\n",
    "    df['1stFlrFrac'] = df['1stFlrSF']/df['GrLivArea']\n",
    "    df['2ndFlrFrac'] = df['2ndFlrSF']/df['GrLivArea']\n",
    "\n",
    "#     df['TotalAreaSF'] = df['GrLivArea']+df['TotalBsmtSF']+df['GarageArea']+df['EnclosedPorch']+df['ScreenPorch']\n",
    "#     df['LivingAreaSF'] = df['1stFlrSF'] + df['2ndFlrSF'] + df['BsmtGLQSF'] + df['BsmtALQSF'] + df['BsmtBLQSF']\n",
    "#     df['StorageAreaSF'] = df['LowQualFinSF'] + df['BsmtRecSF'] + df['BsmtLwQSF'] + df['BsmtUnfSF'] + df['GarageArea']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_meaningful_ordering(df):\n",
    "    # convert some categorical values to numeric scales\n",
    "\n",
    "    #Excellent, Good, Typical, Fair, Poor, None: Convert to 0-5 scale\n",
    "    cols_ExGd = ['ExterQual','ExterCond','BsmtQual','BsmtCond',\n",
    "                 'HeatingQC','KitchenQual','FireplaceQu','GarageQual',\n",
    "                'GarageCond','PoolQC']\n",
    "\n",
    "    dict_ExGd = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n",
    "\n",
    "    for col in cols_ExGd:\n",
    "        df[col].replace(dict_ExGd, inplace=True)\n",
    "\n",
    "    # Remaining columns\n",
    "    df['BsmtExposure'].replace({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0}, inplace=True)\n",
    "\n",
    "    df['CentralAir'].replace({'Y':1,'N':0}, inplace=True)\n",
    "\n",
    "    df['Functional'].replace({'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1,'Sal':0}, inplace=True)\n",
    "\n",
    "    df['GarageFinish'].replace({'Fin':3,'RFn':2,'Unf':1,'None':0}, inplace=True)\n",
    "\n",
    "    df['LotShape'].replace({'Reg':3,'IR1':2,'IR2':1,'IR3':0}, inplace=True)\n",
    "\n",
    "    df['Utilities'].replace({'AllPub':3,'NoSewr':2,'NoSeWa':1,'ELO':0}, inplace=True)\n",
    "\n",
    "    df['LandSlope'].replace({'Gtl':2,'Mod':1,'Sev':0}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dealing_with_zeros(df):\n",
    "    # fraction of zeros in each column\n",
    "    frac_zeros = ((df==0).sum()/len(df))\n",
    "\n",
    "    # no. unique values in each column\n",
    "    n_unique = df.nunique()\n",
    "\n",
    "    # difference between frac. zeros and expected\n",
    "    # frac. zeros if values evenly distributed between\n",
    "    # classes\n",
    "    xs_zeros = frac_zeros - 1/n_unique\n",
    "\n",
    "    # create dataframe and display which columns may be problematic\n",
    "    zero_cols = pd.DataFrame({'frac_zeros':frac_zeros,'n_unique':n_unique,'xs_zeros':xs_zeros})\n",
    "    zero_cols = zero_cols[zero_cols.frac_zeros>0]\n",
    "    zero_cols.sort_values(by='xs_zeros',ascending=False,inplace=True)\n",
    " \n",
    "    #very few properties with Pool or 3SsnPorch\n",
    "    #replace columns with binary indicator\n",
    "    df['HasPool'] = (df['PoolQC']>0).astype(int)\n",
    "    df['Has3SsnPorch'] = (df['3SsnPorch']>0).astype(int)\n",
    "    df.drop(['PoolQC','PoolArea','3SsnPorch'],axis=1,inplace=True)\n",
    "\n",
    "    # 'half' bathrooms - add half value to 'full' bathrooms\n",
    "    df['BsmtFullBath'] = df['BsmtFullBath'] + 0.5*df['BsmtHalfBath']\n",
    "    df['FullBath'] = df['FullBath'] + 0.5*df['HalfBath']\n",
    "    df.drop(['BsmtHalfBath','HalfBath'],axis=1,inplace=True)\n",
    "\n",
    "    # create additional dummy variable for\n",
    "    # continuous variables with a lot of zeros\n",
    "#     dummy_cols = ['LowQualFinSF','2ndFlrSF',\n",
    "#                   'MiscVal','ScreenPorch','WoodDeckSF','OpenPorchSF',\n",
    "#                   'EnclosedPorch','MasVnrArea','GarageArea','Fireplaces',\n",
    "#                   'BsmtGLQSF','BsmtALQSF','BsmtBLQSF','BsmtRecSF',\n",
    "#                   'BsmtLwQSF','BsmtUnfSF','TotalBsmtSF']\n",
    "\n",
    "    dummy_cols = ['MiscVal','ScreenPorch','EnclosedPorch','MasVnrArea','GarageArea','Fireplaces']\n",
    "    for col in dummy_cols:\n",
    "        df['Has' + col] = (df[col] > 0).astype(int)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = reading_data(DTRAIN_PATH)\n",
    "test_data = reading_data(DTEST_PATH)\n",
    "\n",
    "all_data = pd.concat((train_data.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                  test_data.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "all_data = fill_columns_with_NaN(all_data)\n",
    "# all_data = basement_finish_types(all_data)\n",
    "all_data = floor_area(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.0005):\n",
      "0.13699331382727425\n",
      "Lasso:\n",
      "Best: 0.885809 using {'alpha': 0.0005}\n",
      "0.000637 (0.000787) with: {'alpha': 1}\n",
      "0.041406 (0.014399) with: {'alpha': 0.1}\n",
      "0.881784 (0.015851) with: {'alpha': 0.001}\n",
      "0.885809 (0.022454) with: {'alpha': 0.0005}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pre_processing_data(train_data, test_data):\n",
    "\n",
    "    # Removendo as linhas com muitos dados faltantes do conjunto de treino.\n",
    "    # train_data = remove_nan_rows(train_data)\n",
    "\n",
    "    # Concatenando dados de treino e teste para facilitar as operações\n",
    "    # de pré processamento.\n",
    "    all_data = pd.concat((train_data.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test_data.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "    # Preenchendo as colunas.\n",
    "    all_data = fill_columns_with_NaN(all_data)\n",
    "    \n",
    "#     Não funcionou\n",
    "#     all_data = basement_finish_types(all_data)\n",
    "\n",
    "    # Aplicando Log nos preços de venda (valores em distribuição normal).\n",
    "    train_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n",
    "\n",
    "#     numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "#     # Calcula o skewness\n",
    "#     # For normally distributed data, the skewness should be about 0.\n",
    "#     # A skewness value > 0 means that there is more weight in the left tail of the distribution.\n",
    "#     skewed_feats = train_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "#     # Seleciona os valores com skewness > 0.75\n",
    "#     skewed_feats = skewed_feats[skewed_feats > 0.75].index\n",
    "\n",
    "#     # Aplicando log nos valores com skewness maior que 0.75\n",
    "#     all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    all_data = floor_area(all_data)\n",
    "    all_data = categorical_meaningful_ordering(all_data)\n",
    "    all_data = dealing_with_zeros(all_data)\n",
    "\n",
    "        # extract names of numeric columns\n",
    "    dtypes = all_data.dtypes\n",
    "    cols_numeric = dtypes[dtypes != object].index.tolist()\n",
    "\n",
    "    # MSubClass should be treated as categorical\n",
    "    cols_numeric.remove('MSSubClass')\n",
    "    \n",
    "    # choose any numeric column with less than 13 values to be\n",
    "    # \"discrete\". 13 chosen to include months of the year.\n",
    "    # other columns \"continuous\"\n",
    "    col_nunique = dict()\n",
    "\n",
    "    for col in cols_numeric:\n",
    "        col_nunique[col] = all_data[col].nunique()\n",
    "\n",
    "    col_nunique = pd.Series(col_nunique)\n",
    "\n",
    "    cols_discrete = col_nunique[col_nunique<13].index.tolist()\n",
    "    cols_continuous = col_nunique[col_nunique>=13].index.tolist()\n",
    "\n",
    "    # normalise numeric columns\n",
    "    scale_cols = [col for col in cols_numeric if col!='SalePrice']\n",
    "\n",
    "    all_data[scale_cols] = all_data[scale_cols].apply(scale_minmax, axis=0)\n",
    "    \n",
    "    \n",
    "    # Converte dados categorigos em dummy indicators\n",
    "    all_data = pd.get_dummies(all_data)\n",
    "\n",
    "    # Preenchendo os valores em branco com a média.\n",
    "    # all_data = all_data.fillna(all_data.mean())\n",
    "    # Preenchendo os valores em branco com a mediana.\n",
    "    # all_data = all_data.fillna(all_data.median())\n",
    "    # Preenchendo os valores em branco com a moda.\n",
    "    # all_data = all_data.fillna(all_data.mode().T)\n",
    "\n",
    "\n",
    "    # Dados para treinamento e teste após pré processamento.\n",
    "    x_train = all_data[:train_data.shape[0]]\n",
    "    x_test = all_data[train_data.shape[0]:]\n",
    "    y_train = train_data.SalePrice\n",
    "\n",
    "    return (x_train, y_train, x_test)\n",
    "\n",
    "\n",
    "train_data = reading_data(DTRAIN_PATH)\n",
    "test_data = reading_data(DTEST_PATH)\n",
    "\n",
    "train_x, train_y, test_x = pre_processing_data(train_data, test_data)\n",
    "\n",
    "\n",
    "print('Lasso(alpha=0.0005):')\n",
    "print(rmse_cv(Lasso(alpha=0.0005), train_x, train_y).mean())\n",
    "# print(rmse_cv(Ridge(alpha=5), train_x, train_y).mean())\n",
    "grid_search_lasso(train_x, train_y)\n",
    "# grid_search_ridge(train_x, train_y)\n",
    "print()\n",
    "# Modelo XBG\n",
    "# print('XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):')\n",
    "# print(rmse_cv(XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1), train_x, train_y).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.0005):\n",
      "0.12162855645714306\n",
      "Lasso:\n",
      "Best: 0.901133 using {'alpha': 0.0005}\n",
      "0.545850 (0.032529) with: {'alpha': 1}\n",
      "0.734174 (0.027527) with: {'alpha': 0.1}\n",
      "0.897713 (0.017699) with: {'alpha': 0.001}\n",
      "0.901133 (0.017624) with: {'alpha': 0.0005}\n",
      "\n",
      "\n",
      "Ridge:\n",
      "Best: 0.895923 using {'alpha': 5}\n",
      "0.894325 (0.016889) with: {'alpha': 1}\n",
      "0.895842 (0.016387) with: {'alpha': 3}\n",
      "0.895923 (0.016170) with: {'alpha': 5}\n",
      "0.895253 (0.015935) with: {'alpha': 10}\n",
      "0.894283 (0.015782) with: {'alpha': 15}\n",
      "0.891200 (0.015413) with: {'alpha': 30}\n",
      "0.887570 (0.015049) with: {'alpha': 50}\n",
      "0.883860 (0.014743) with: {'alpha': 75}\n",
      "\n",
      "\n",
      "\n",
      "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
      "0.12184203429783551\n"
     ]
    }
   ],
   "source": [
    "train_data = reading_data(DTRAIN_PATH)\n",
    "test_data = reading_data(DTEST_PATH)\n",
    "\n",
    "train_x, train_y, test_x = pre_processing_data(train_data, test_data)\n",
    "\n",
    "\n",
    "print('Lasso(alpha=0.0005):')\n",
    "print(rmse_cv(Lasso(alpha=0.0005), train_x, train_y).mean())\n",
    "# print(rmse_cv(Ridge(alpha=10), train_x, train_y).mean())\n",
    "grid_search_lasso(train_x, train_y)\n",
    "grid_search_ridge(train_x, train_y)\n",
    "print()\n",
    "# Modelo XBG\n",
    "print('XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):')\n",
    "print(rmse_cv(XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1), train_x, train_y).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Electrical', 1),\n",
       " ('MasVnrType', 8),\n",
       " ('MasVnrArea', 8),\n",
       " ('BsmtQual', 37),\n",
       " ('BsmtCond', 37),\n",
       " ('BsmtFinType1', 37),\n",
       " ('BsmtExposure', 38),\n",
       " ('BsmtFinType2', 38),\n",
       " ('GarageType', 81),\n",
       " ('GarageYrBlt', 81),\n",
       " ('GarageFinish', 81),\n",
       " ('GarageQual', 81),\n",
       " ('GarageCond', 81),\n",
       " ('LotFrontage', 259),\n",
       " ('FireplaceQu', 690),\n",
       " ('Fence', 1179),\n",
       " ('Alley', 1369),\n",
       " ('MiscFeature', 1406),\n",
       " ('PoolQC', 1453)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_data\n",
    "nans_cols = {}\n",
    "\n",
    "for column_name in df.columns:\n",
    "    column = df[column_name]\n",
    "    nans = column.isnull().sum()\n",
    "    if(nans > 0):\n",
    "        nans_cols[column_name] = nans\n",
    "\n",
    "# [print(v) for v in nans_cols]\n",
    "import operator\n",
    "sorted(nans_cols.items(),  key=operator.itemgetter(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Default\n",
    "\n",
    "Lasso(alpha=0.0005):\n",
    "0.1211865670995667\n",
    "\n",
    "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
    "0.12249088341435015\n",
    "\n",
    "## Removing nans values per col (upper_bound = 0.3)\n",
    "\n",
    "Lasso(alpha=0.0005):\n",
    "0.12082472393696742\n",
    "\n",
    "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
    "0.12304465867881136\n",
    "\n",
    "## Removing nans values per row and cols\n",
    "Lasso(alpha=0.0005):\n",
    "0.11805243912299515\n",
    "\n",
    "## Using Median\n",
    "Lasso(alpha=0.0005):\n",
    "0.1180499078769596\n",
    "\n",
    "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
    "0.116734983343264\n",
    "\n",
    "## Using Mode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
