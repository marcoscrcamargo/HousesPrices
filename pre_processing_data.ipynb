{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho dos dados.\n",
    "DTRAIN_PATH = \"data/train.csv\"\n",
    "DTEST_PATH = \"data/test.csv\"\n",
    "\n",
    "# Funções auxiliares.\n",
    "def reading_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def rmse_cv(model, x_train, y_train, k_folds=10):\n",
    "    return np.sqrt(-cross_val_score(model, x_train, y_train,\n",
    "        scoring=\"neg_mean_squared_error\", cv=k_folds))\n",
    "\n",
    "def grid_search(model, params, train_x, train_y):\n",
    "\t# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\t# gridsearch = GridSearchCV(model, param_grid=params, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "\n",
    "\tgridsearch = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "\tgridresult = gridsearch.fit(train_x, train_y)\n",
    "\n",
    "\treturn gridresult\n",
    "\n",
    "def print_grid_result(name, grid_result):\n",
    "\tprint(name + \":\")\n",
    "\t# sumarize results\n",
    "\tprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\tmeans = grid_result.cv_results_['mean_test_score']\n",
    "\tstds = grid_result.cv_results_['std_test_score']\n",
    "\tparams = grid_result.cv_results_['params']\n",
    "\tfor mean, stdev, param in zip(means, stds, params):\n",
    "\t\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\tprint(\"\\n\")\n",
    "\n",
    "\n",
    "def grid_search_lasso(train_x, train_y):\n",
    "\tparams = {'alpha':(1, 0.1, 0.001, 0.0005)}\n",
    "\n",
    "\tgrid_result = grid_search(Lasso(), params, train_x, train_y)\n",
    "\tprint_grid_result(\"Lasso\", grid_result)\n",
    "\n",
    "def grid_search_ridge(train_x, train_y):\n",
    "\tparams = {'alpha':(1, 3, 5, 10, 15, 30, 50, 75)}\n",
    "\n",
    "\tgrid_result = grid_search(Ridge(), params, train_x, train_y)\n",
    "\tprint_grid_result(\"Ridge\", grid_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções interessantes\n",
    "# all_data.head()\n",
    "# all_data.tail()\n",
    "# all_data.info()\n",
    "# all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns that the number of nans values is greater than bound=0.3 \n",
    "def remove_nan_columns(df, bound=0.3):\n",
    "    for column_name in df.columns:\n",
    "        column = df[column_name]\n",
    "        nan_percentage = column.isnull().sum()/column.size\n",
    "        if(nan_percentage > bound):\n",
    "            df = df.drop(columns=[column_name])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows that the number of nans values is greater than bound=0.1 (more than 10 values null)\n",
    "def remove_nan_rows(df, bound=0.1):\n",
    "    row_size = train_data.shape[1]\n",
    "    for index, row in df.iterrows():\n",
    "        rownan_percentage = row.isnull().sum()/row_size\n",
    "        if (rownan_percentage >= bound ):\n",
    "            df = df.drop(index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing redundance using drop_duplicates and correlation\n",
    "def remove_redundance(df, bound=0.0005):\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    corr = df.corr().abs()\n",
    "    corr = corr[corr!= 1]\n",
    "    print(corr.min() < bound)\n",
    "    remove = np.empty(0)\n",
    "    columns = corr.columns\n",
    "    for index, row in corr.iterrows():\n",
    "        for col in columns:\n",
    "            if (row[col] < bound):\n",
    "                remove = np.append(remove, col)\n",
    "    print(df.shape)\n",
    "    df = df.drop(columns=np.unique(remove))\n",
    "    print(df.shape)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.0005):\n",
      "0.1180499078769596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pre_processing_data(train_data, test_data):\n",
    "\n",
    "    # Removendo as linhas com muitos dados faltantes do conjunto de treino.\n",
    "    train_data = remove_nan_rows(train_data)\n",
    "\n",
    "    # Concatenando dados de treino e teste para facilitar as operações\n",
    "    # de pré processamento.\n",
    "    all_data = pd.concat((train_data.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                      test_data.loc[:,'MSSubClass':'SaleCondition']))\n",
    "\n",
    "    # Removendo as colunas com muitos dados ausentes.\n",
    "    all_data = remove_nan_columns(all_data)\n",
    "\n",
    "#   all_data = remove_redundance(all_data, bound=0.00001)\n",
    "    \n",
    "    # Aplicando Log nos preços de venda (valores em distribuição normal).\n",
    "    train_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n",
    "\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "    # Calcula o skewness\n",
    "    # For normally distributed data, the skewness should be about 0.\n",
    "    # A skewness value > 0 means that there is more weight in the left tail of the distribution.\n",
    "    skewed_feats = train_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "    # Seleciona os valores com skewness > 0.75\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75].index\n",
    "\n",
    "    # Aplicando log nos valores com skewness maior que 0.75\n",
    "    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "\n",
    "    # Converte dados categorigos em dummy indicators\n",
    "    all_data = pd.get_dummies(all_data)\n",
    "\n",
    "    all_data.shape\n",
    "    # Preenchendo os valores em branco com a média.\n",
    "#     all_data = all_data.fillna(all_data.mean())\n",
    "    # Preenchendo os valores em branco com a mediana.\n",
    "    all_data = all_data.fillna(all_data.median())\n",
    "    # Preenchendo os valores em branco com a moda.\n",
    "#     all_data = all_data.fillna(all_data.mode().T)\n",
    "\n",
    "\n",
    "    # Dados para treinamento e teste após pré processamento.\n",
    "    x_train = all_data[:train_data.shape[0]]\n",
    "    x_test = all_data[train_data.shape[0]:]\n",
    "    y_train = train_data.SalePrice\n",
    "\n",
    "    return (x_train, y_train, x_test)\n",
    "\n",
    "\n",
    "train_data = reading_data(DTRAIN_PATH)\n",
    "test_data = reading_data(DTEST_PATH)\n",
    "\n",
    "train_x, train_y, test_x = pre_processing_data(train_data, test_data)\n",
    "\n",
    "\n",
    "print('Lasso(alpha=0.0005):')\n",
    "print(rmse_cv(Lasso(alpha=0.0005), train_x, train_y).mean())\n",
    "# print(rmse_cv(Ridge(alpha=5), train_x, train_y).mean())\n",
    "# grid_search_lasso(train_x, train_y)\n",
    "# grid_search_ridge(train_x, train_y)\n",
    "print()\n",
    "# Modelo XBG\n",
    "# print('XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):')\n",
    "# print(rmse_cv(XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1), train_x, train_y).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso:\n",
      "Best: 0.897062 using {'alpha': 0.0005}\n",
      "0.376606 (0.012845) with: {'alpha': 1}\n",
      "0.691365 (0.003917) with: {'alpha': 0.1}\n",
      "0.894183 (0.019403) with: {'alpha': 0.001}\n",
      "0.897062 (0.020630) with: {'alpha': 0.0005}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = reading_data(DTRAIN_PATH)\n",
    "test_data = reading_data(DTEST_PATH)\n",
    "\n",
    "cp_train_data = train_data\n",
    "\n",
    "train_x, train_y, test_x = pre_processing_data(train_data, test_data)\n",
    "\n",
    "grid_search_lasso(train_x, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSSubClass        True\n",
      "LotFrontage      False\n",
      "LotArea          False\n",
      "OverallQual      False\n",
      "OverallCond      False\n",
      "YearBuilt         True\n",
      "YearRemodAdd      True\n",
      "MasVnrArea       False\n",
      "BsmtFinSF1       False\n",
      "BsmtFinSF2        True\n",
      "BsmtUnfSF        False\n",
      "TotalBsmtSF      False\n",
      "1stFlrSF         False\n",
      "2ndFlrSF         False\n",
      "LowQualFinSF      True\n",
      "GrLivArea        False\n",
      "BsmtFullBath      True\n",
      "BsmtHalfBath      True\n",
      "FullBath         False\n",
      "HalfBath          True\n",
      "BedroomAbvGr      True\n",
      "KitchenAbvGr      True\n",
      "TotRmsAbvGrd     False\n",
      "Fireplaces       False\n",
      "GarageYrBlt      False\n",
      "GarageCars       False\n",
      "GarageArea       False\n",
      "WoodDeckSF       False\n",
      "OpenPorchSF      False\n",
      "EnclosedPorch     True\n",
      "3SsnPorch        False\n",
      "ScreenPorch      False\n",
      "PoolArea          True\n",
      "MiscVal           True\n",
      "MoSold            True\n",
      "YrSold            True\n",
      "dtype: bool\n",
      "(2811, 74)\n",
      "(2811, 59)\n",
      "Lasso(alpha=0.0005):\n",
      "0.11790325613246747\n",
      "0.120754448103974\n",
      "\n",
      "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
      "0.11732453542503929\n"
     ]
    }
   ],
   "source": [
    "train_data = reading_data(DTRAIN_PATH)\n",
    "test_data = reading_data(DTEST_PATH)\n",
    "\n",
    "train_x, train_y, test_x = pre_processing_data(train_data, test_data)\n",
    "\n",
    "\n",
    "print('Lasso(alpha=0.0005):')\n",
    "print(rmse_cv(Lasso(alpha=0.0005), train_x, train_y).mean())\n",
    "print(rmse_cv(Ridge(alpha=5), train_x, train_y).mean())\n",
    "# grid_search_lasso(train_x, train_y)\n",
    "# grid_search_ridge(train_x, train_y)\n",
    "print()\n",
    "# Modelo XBG\n",
    "print('XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):')\n",
    "print(rmse_cv(XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1), train_x, train_y).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Electrical', 1),\n",
       " ('MasVnrType', 8),\n",
       " ('MasVnrArea', 8),\n",
       " ('BsmtQual', 37),\n",
       " ('BsmtCond', 37),\n",
       " ('BsmtFinType1', 37),\n",
       " ('BsmtExposure', 38),\n",
       " ('BsmtFinType2', 38),\n",
       " ('GarageType', 81),\n",
       " ('GarageYrBlt', 81),\n",
       " ('GarageFinish', 81),\n",
       " ('GarageQual', 81),\n",
       " ('GarageCond', 81),\n",
       " ('LotFrontage', 259),\n",
       " ('FireplaceQu', 690),\n",
       " ('Fence', 1179),\n",
       " ('Alley', 1369),\n",
       " ('MiscFeature', 1406),\n",
       " ('PoolQC', 1453)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_data\n",
    "nans_cols = {}\n",
    "\n",
    "for column_name in df.columns:\n",
    "    column = df[column_name]\n",
    "    nans = column.isnull().sum()\n",
    "    if(nans > 0):\n",
    "        nans_cols[column_name] = nans\n",
    "\n",
    "# [print(v) for v in nans_cols]\n",
    "import operator\n",
    "sorted(nans_cols.items(),  key=operator.itemgetter(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Default\n",
    "\n",
    "Lasso(alpha=0.0005):\n",
    "0.1211865670995667\n",
    "\n",
    "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
    "0.12249088341435015\n",
    "\n",
    "## Removing nans values per col (upper_bound = 0.3)\n",
    "\n",
    "Lasso(alpha=0.0005):\n",
    "0.12082472393696742\n",
    "\n",
    "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
    "0.12304465867881136\n",
    "\n",
    "## Removing nans values per row and cols\n",
    "Lasso(alpha=0.0005):\n",
    "0.11805243912299515\n",
    "\n",
    "## Using Median\n",
    "Lasso(alpha=0.0005):\n",
    "0.1180499078769596\n",
    "\n",
    "XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1):\n",
    "0.116734983343264\n",
    "\n",
    "## Using Mode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
